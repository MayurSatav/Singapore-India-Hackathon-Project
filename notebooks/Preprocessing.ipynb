{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "stopwords = list(STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"data_trial_1.csv\",encoding='latin-1')\n",
    "raw_data.columns = [i for i in range(len(raw_data.columns))]\n",
    "raw_data = raw_data[[0,4,5]].dropna()\n",
    "raw_data.head()\n",
    "\n",
    "## doesn't work for some reason\n",
    "# raw_data = pd.read_csv(\"data_trial_1.csv\",encoding='latin-1')\n",
    "# raw_data.columns = [['sentiment', 'id','date','query','user','post']]\n",
    "# raw_data = raw_data[['sentiment','user','post']]\n",
    "# raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\"aren't\": 'are not',\n",
    "                \"ain't\": \"is not\",\n",
    "                 \"she'd\": 'she would',\n",
    "                 \"can't\": 'cannot',\n",
    "                 \"shouldn't\": 'should not',\n",
    "                 \"couldn't\": 'could not',\n",
    "                 \"that's\": 'that is',\n",
    "                 \"didn't\": 'did not',\n",
    "                 \"there's\": 'there is',\n",
    "                 \"don't\": 'do not',\n",
    "                 \"they're\": 'they are',\n",
    "                 \"doesn't\": 'does not',\n",
    "                 \"they've\": 'they have',\n",
    "                 \"hadn't\": 'had not',\n",
    "                 \"they'll\": 'they will',\n",
    "                 \"haven't\": 'have not',\n",
    "                 \"they'd\": 'they had',\n",
    "                 \"he's\": 'he has',\n",
    "                 \"wasn't\": 'was not',\n",
    "                 \"he'll\": 'he will',\n",
    "                 \"we're\": 'we are',\n",
    "                 \"he'd\": 'he would',\n",
    "                 \"we've\": 'we have',\n",
    "                 \"here's\": 'here is',\n",
    "                 \"we'll\": 'we will',\n",
    "                 \"i'm\": 'i am',\n",
    "                 \"we'd\": 'we had',\n",
    "                 \"i've\": 'i have',\n",
    "                 \"i'll\": 'i will',\n",
    "                 \"weren't\": 'were not',\n",
    "                 \"i'd\": 'i had',\n",
    "                 \"what's\": 'what is',\n",
    "                 \"where's\": 'where is',\n",
    "                 \"isn't\": 'is not',\n",
    "                 \"who's\": 'who is',\n",
    "                 \"it's\": 'it has',\n",
    "                 \"who'll\": 'who will',\n",
    "                 \"won't\": 'will not',\n",
    "                 \"wouldn't\": 'would not',\n",
    "                 \"it'll\": 'it will',\n",
    "                 \"you're\": 'you are',\n",
    "                 \"mustn't\": 'must not',\n",
    "                 \"you've\": 'you have',\n",
    "                 \"she's\": 'she has',\n",
    "                 \"you'll\": 'you will',\n",
    "                 \"you'd\": 'you had',\n",
    "                 \"she'll\": 'she will',\n",
    "                 \"gon na\": \"going to\",\n",
    "                 \"gonna\" : \"going to\",\n",
    "                 \"wan na\": \"want to\",\n",
    "                 \"wanna\":\"want to\",\n",
    "                 \" u \": \" you \",\n",
    "                 \"got ta\":\"got to\",\n",
    "                 \"gotta\" :\"got to\",\n",
    "                 \" r \" : \" are \"}\n",
    "\n",
    "# common_word_file = open('long_stopwords.txt','r')\n",
    "# common_words = []\n",
    "# for line_ in common_word_file:\n",
    "#     common_words.append(line_.strip())\n",
    "\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "common_words = list(spacy.lang.en.stop_words.STOP_WORDS)\n",
    "\n",
    "def to_ascii (text):\n",
    "    \n",
    "    ret = ''\n",
    "    for char in text:\n",
    "        if char in \"!\\\"#$%&\\'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]^_`abcdefghijklmnopqrstuvwxyz{|}~ \":\n",
    "            ret+=char\n",
    "        else:\n",
    "            ret+=\" \"+char\n",
    "    return ret\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    if type(text)==str:\n",
    "        text = to_ascii(text)\n",
    "        text = text.lower()\n",
    "\n",
    "        text = text.replace(\";\",\" \") \n",
    "        text = text.replace(\"&\",\" and \")\n",
    "\n",
    "        text = re.sub(\"[?!]+\",\".\",text) # should these be filtered out?\n",
    "    #     text = re.sub(\"[\\W]+\",\" \",text)\n",
    "        text = re.sub(\"@[\\w]*\",\" \",text)\n",
    "        text = re.sub(\"\\#[\\w]*\",\" \",text)\n",
    "        text = re.sub(\"http:[^ ]*\",\" \",text)\n",
    "        text = re.sub(\"[\\.]+\",\".\",text)\n",
    "        text = re.sub(\"[()]\",\" \",text)\n",
    "        text = re.sub(\"[\\s]+\",\" \",text)\n",
    "        text = re.sub(\"[l]{3,}\",\"l\",text)\n",
    "        text = re.sub(\"[y]{2,}\",\"y\",text)\n",
    "        text = re.sub(\"[o]{3,}\",\"o\",text)\n",
    "#         text = re.sub(\"[a-z]{2,}\",\"\\0\",text)\n",
    "\n",
    "        for contraction in contractions.keys():\n",
    "            contraction_alpha = \" \"+ \"\".join(re.findall(\"[a-zA-Z]+\", contraction))+\" \"\n",
    "            text = text.replace(contraction,contractions[contraction]) \n",
    "            text = text.replace(contraction_alpha,\" \" + contractions[contraction] + \" \") \n",
    "        \n",
    "        text = text.replace(\" ta \",\" \") \n",
    "        text = text.replace(\" quot \",\" \") \n",
    "        return text\n",
    "    \n",
    "    else:\n",
    "        ret = []\n",
    "        for item in text:\n",
    "            ret.append(clean_text(item))\n",
    "        return ret\n",
    "\n",
    "def filter_and_tokenise(sents):\n",
    "    ret = []\n",
    "    for sent in tqdm(sents):\n",
    "        res_sent = []\n",
    "        for word in word_tokenize(sent):\n",
    "            if word not in stopwords and word.isalpha() and len(word)>1:\n",
    "#             if word.isalpha():\n",
    "                res_sent.append(word)\n",
    "        ret.append(res_sent)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = raw_data.iloc[:1000].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data[6] = clean_text(raw_data [5])\n",
    "raw_data[7] = filter_and_tokenise(raw_data [6])\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data.to_pickle(\"df_saved.pkl\")\n",
    "# raw_data = pd.read_pickle(\"df_saved.pkl\")\n",
    "# raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_corpus = list(raw_data[7])\n",
    "\n",
    "bigram_list = []\n",
    "ctr=0\n",
    "for ele in tqdm(split_corpus):\n",
    "    t_bigrams = ngrams(ele,2)\n",
    "    for ele in t_bigrams:\n",
    "        if ele[0] in common_words or ele[1] in common_words:\n",
    "            ctr+=1\n",
    "            continue\n",
    "        elif len(ele[0])>2 and len(ele[1])>2 and ele[0]!=ele[1]:\n",
    "            bigram_list.append((ele[0],ele[1]))\n",
    "            \n",
    "bigram_df = pd.DataFrame(pd.Series(bigram_list).value_counts()).reset_index()\n",
    "bigram_df.columns = ['bigram','frequency']\n",
    "bigram_df = bigram_df[bigram_df['frequency']>10]\n",
    "print (len(bigram_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_df.iloc[5000:6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_bigrams_to_replace = 6000\n",
    "\n",
    "sent_corpus = []\n",
    "for ele in split_corpus:\n",
    "    t_sent = ' '.join(ele)\n",
    "    sent_corpus.append(t_sent)\n",
    "\n",
    "print(len(sent_corpus))\n",
    "\n",
    "merged_corpus_all = '----------'.join(sent_corpus)\n",
    "bigram_list = list(bigram_df['bigram'])\n",
    "\n",
    "ctr=0\n",
    "for ele in tqdm(bigram_list[:num_of_bigrams_to_replace]):\n",
    "    ctr+=1\n",
    "    lookup_key = \" \"+ ele[0]+' '+ele[1]+\" \"\n",
    "    replace_key = \" \"+ ele[0]+'_'+ele[1]+\" \"\n",
    "    merged_corpus_all = merged_corpus_all.replace(lookup_key,replace_key)\n",
    "\n",
    "sent_list = merged_corpus_all.split('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data[8] = sent_list\n",
    "raw_data[8] = raw_data[8].apply(str.split)\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(raw_data[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.to_pickle(\"df_saved.pkl\")\n",
    "# raw_data = pd.read_pickle(\"df_saved.pkl\")\n",
    "# raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = raw_data[raw_data[0]==4]\n",
    "print (positive_tweets.head(),\"\\n\")\n",
    "positive_tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_tweets = raw_data[raw_data[0]==0]\n",
    "print (negative_tweets.head(),\"\\n\")\n",
    "negative_tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='black',\n",
    "        stopwords=stopwords,\n",
    "        max_words=200,\n",
    "        max_font_size=40, \n",
    "        scale=3,\n",
    "        random_state=1 # chosen at random by flipping a coin; it was heads\n",
    "        ).generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(15, 15))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(negative_tweets[6],'Most Common Words from the negative corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(positive_tweets[6],'Most Common Words from the positive corpus')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
